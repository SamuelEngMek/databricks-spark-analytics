{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fded1d-2ca6-485e-94e9-3c5488255936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Análise de Estoque e Previsão de Demanda\n",
    "\n",
    "Este notebook apresenta uma análise dos dados relacionados à gestão de estoque e previsão de demanda de produtos em diferentes locais de loja. Utilizando PySpark, o objetivo principal é avaliar a quantidade de produtos vendidos, a quantidade em estoque, o impacto de promoções e as condições meteorológicas sobre a defasagem entre a demanda real e a previsão.\n",
    "\n",
    "## Etapas abordadas:\n",
    "- **Carregamento e pré-processamento dos dados**.\n",
    "- **Identificação de produtos com estoque insuficiente**.\n",
    "- **Cálculo da defasagem entre a demanda e o estoque disponível**.\n",
    "- **Análise do impacto de promoções e variáveis externas** (como condições meteorológicas e feriados) na defasagem de estoque.\n",
    "- **Cálculo da precisão das previsões de demanda** e a comparação entre acertos e erros na previsão.\n",
    "\n",
    "O notebook oferece uma visão geral das métricas de desempenho de estoque, fornecendo insights valiosos para a gestão eficiente de inventário e estratégias de reposição.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760fa5aa-d594-4061-a15e-bb0481a7e110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importação de Bibliotecas\n",
    "\n",
    "Neste primeiro passo, importamos as bibliotecas necessárias para manipulação de dados:\n",
    "\n",
    "- **Pandas**: para operações de manipulação de dados em formato de DataFrame.\n",
    "- **NumPy**: para operações matemáticas e manipulação de arrays.\n",
    "- **PySpark**: para processamento de dados distribuídos com Apache Spark, utilizando funções da biblioteca PySpark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e98fdf9-1483-4a65-8611-1bf34b9758bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6634bdf-974c-40b3-b75b-6a0a03e0742a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carregamento dos Dados como PySpark DataFrame\n",
    "\n",
    "Neste passo, carregamos os dados de um arquivo **Parquet** utilizando o PySpark. O formato Parquet é eficiente para armazenar dados estruturados e pode ser lido diretamente com o PySpark.\n",
    "\n",
    "Usamos o método `spark.read.parquet` para carregar os dados no formato Parquet e armazená-los em um DataFrame do PySpark. Também configuramos as opções `header=True` e `inferSchema=True` para garantir que o cabeçalho seja interpretado e que o tipo de dados seja inferido automaticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46cb6c7-63c8-437e-b7a4-4fac0cc93e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- transaction_id: integer (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- product_name: string (nullable = true)\n |-- category: string (nullable = true)\n |-- quantity_sold: integer (nullable = true)\n |-- unit_price: double (nullable = true)\n |-- transaction_date: timestamp (nullable = true)\n |-- store_id: integer (nullable = true)\n |-- store_location: string (nullable = true)\n |-- inventory_level: integer (nullable = true)\n |-- reorder_point: integer (nullable = true)\n |-- reorder_quantity: integer (nullable = true)\n |-- supplier_id: integer (nullable = true)\n |-- supplier_lead_time: integer (nullable = true)\n |-- customer_age: integer (nullable = true)\n |-- customer_gender: string (nullable = true)\n |-- customer_income: double (nullable = true)\n |-- customer_loyalty_level: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- promotion_applied: boolean (nullable = true)\n |-- promotion_type: string (nullable = true)\n |-- weather_conditions: string (nullable = true)\n |-- holiday_indicator: boolean (nullable = true)\n |-- weekday: string (nullable = true)\n |-- stockout_indicator: boolean (nullable = true)\n |-- forecasted_demand: integer (nullable = true)\n |-- actual_demand: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Carregar os dados como PySpark DataFrame \n",
    "df = spark.read.parquet(\"/tmp/dados_transformed.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "# Exibir o esquema do DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa416fa-a19a-4ff5-91c9-97f2c0cccbd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verificação de Valores Nulos nas Colunas\n",
    "\n",
    "Após carregar os dados, é essencial verificar se há valores nulos em qualquer uma das colunas do DataFrame. A presença de valores nulos pode afetar análises subsequentes, por isso é importante tratá-los adequadamente.\n",
    "\n",
    "Neste passo, utilizamos a função `select` em conjunto com `F.count` e `F.when` do PySpark para contar o número de valores nulos em cada coluna. A função `F.col(c).isNull()` verifica se o valor da coluna `c` é nulo. O resultado é exibido utilizando o método `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6051fe9d-d735-48e6-b6c8-d31a388af7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+------------+--------+-------------+----------+----------------+--------+--------------+---------------+-------------+----------------+-----------+------------------+------------+---------------+---------------+----------------------+--------------+-----------------+--------------+------------------+-----------------+-------+------------------+-----------------+-------------+\n|transaction_id|customer_id|product_id|product_name|category|quantity_sold|unit_price|transaction_date|store_id|store_location|inventory_level|reorder_point|reorder_quantity|supplier_id|supplier_lead_time|customer_age|customer_gender|customer_income|customer_loyalty_level|payment_method|promotion_applied|promotion_type|weather_conditions|holiday_indicator|weekday|stockout_indicator|forecasted_demand|actual_demand|\n+--------------+-----------+----------+------------+--------+-------------+----------+----------------+--------+--------------+---------------+-------------+----------------+-----------+------------------+------------+---------------+---------------+----------------------+--------------+-----------------+--------------+------------------+-----------------+-------+------------------+-----------------+-------------+\n|             0|          0|         0|           0|       0|            0|         0|               0|       0|             0|              0|            0|               0|          0|                 0|           0|              0|              0|                     0|             0|                0|             0|                 0|                0|      0|                 0|                0|            0|\n+--------------+-----------+----------+------------+--------+-------------+----------+----------------+--------+--------------+---------------+-------------+----------------+-----------+------------------+------------+---------------+---------------+----------------------+--------------+-----------------+--------------+------------------+-----------------+-------+------------------+-----------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores nulos nas colunas\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b87c10-6ced-41cb-abeb-38045cf9819f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Definição das Colunas a Serem Mantidas para Gestão de Estoque e Previsão de Demanda\n",
    "\n",
    "Neste passo, selecionamos as colunas relevantes para o processo de gestão de estoque e previsão de demanda. Essas colunas contêm informações cruciais para entender o comportamento do estoque, as vendas, as promoções, e as condições que impactam a demanda dos produtos.\n",
    "\n",
    "A lista `columns_to_keep` inclui as colunas a serem mantidas no DataFrame para análises futuras:\n",
    "\n",
    "- **product_id**: Identificador único do produto.\n",
    "- **quantity_sold**: Quantidade de unidades vendidas.\n",
    "- **inventory_level**: Nível atual de estoque.\n",
    "- **reorder_point**: Ponto de reposição do estoque.\n",
    "- **reorder_quantity**: Quantidade a ser reposta quando o estoque atingir o ponto de reposição.\n",
    "- **store_id**: Identificador único da loja.\n",
    "- **store_location**: Localização da loja.\n",
    "- **forecasted_demand**: Demanda prevista para o produto.\n",
    "- **promotion_applied**: Indica se a promoção foi aplicada ao produto.\n",
    "- **promotion_type**: Tipo da promoção aplicada (ex: desconto, promoção combinada, etc.).\n",
    "- **weather_conditions**: Condições climáticas que podem influenciar a demanda.\n",
    "- **holiday_indicator**: Indicador de feriado, que pode afetar as vendas.\n",
    "- **weekday**: Dia da semana, relevante para padrões de vendas.\n",
    "- **actual_demand**: Demanda real observada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b30200-9858-4a27-9bb6-d3ec60d18fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Colunas a serem mantidas para Gestão de Estoque e Previsão de Demanda\n",
    "columns_to_keep = [\n",
    "    'product_id', 'quantity_sold', 'inventory_level', 'reorder_point', 'reorder_quantity', \n",
    "    'store_id', 'store_location', 'forecasted_demand', 'promotion_applied', 'promotion_type', \n",
    "    'weather_conditions', 'holiday_indicator', 'weekday', 'actual_demand'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d33053-fa60-4c1a-bc6a-5c52e8458a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtragem das Colunas Relevantes\n",
    "\n",
    "Após definir as colunas importantes para a gestão de estoque e previsão de demanda, realizamos a filtragem do DataFrame para manter apenas essas colunas. Esse passo é crucial para simplificar o DataFrame e focar nas variáveis essenciais para a análise subsequente.\n",
    "\n",
    "Utilizamos o método `select` para selecionar as colunas definidas na lista `columns_to_keep` e criar um novo DataFrame `df_filtered`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc54a0a3-871e-4e9c-ac36-c3bf3b553372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------+-------------+----------------+--------+---------------+-----------------+-----------------+--------------+------------------+-----------------+---------+-------------+\n|product_id|quantity_sold|inventory_level|reorder_point|reorder_quantity|store_id| store_location|forecasted_demand|promotion_applied|promotion_type|weather_conditions|holiday_indicator|  weekday|actual_demand|\n+----------+-------------+---------------+-------------+----------------+--------+---------------+-----------------+-----------------+--------------+------------------+-----------------+---------+-------------+\n|       405|            5|             77|          108|             267|       2|Los Angeles, CA|              210|            false|          None|            Stormy|            false|   Friday|          299|\n|       987|            1|            447|           94|             215|      17|Los Angeles, CA|              210|             true|          None|            Stormy|            false|Wednesday|          246|\n|       165|            3|            204|          146|             129|       5|      Miami, FL|              306|            false|          None|             Sunny|             true| Saturday|          315|\n|       516|            3|            126|           64|             118|       3|      Miami, FL|              494|             true|          None|            Cloudy|            false|   Sunday|          357|\n|       472|            3|            160|          140|             103|       7|     Dallas, TX|              377|             true|          BOGO|             Sunny|             true| Saturday|          413|\n+----------+-------------+---------------+-------------+----------------+--------+---------------+-----------------+-----------------+--------------+------------------+-----------------+---------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filtrar as colunas relevantes\n",
    "df_filtered = df.select(*columns_to_keep)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame filtrado para conferir\n",
    "df_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30f90c7-bae0-47a0-93a4-481ac8279b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exibição do Esquema do DataFrame Filtrado\n",
    "\n",
    "Após realizar a filtragem das colunas relevantes, é importante verificar novamente a estrutura do DataFrame filtrado para garantir que as colunas selecionadas estão presentes e que os tipos de dados estão corretos.\n",
    "\n",
    "Utilizamos o método `printSchema()` para exibir o esquema do DataFrame filtrado, que nos mostra as colunas e seus respectivos tipos de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92f3044-8e0c-435f-bab5-90a80a13cd28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- product_id: integer (nullable = true)\n |-- quantity_sold: integer (nullable = true)\n |-- inventory_level: integer (nullable = true)\n |-- reorder_point: integer (nullable = true)\n |-- reorder_quantity: integer (nullable = true)\n |-- store_id: integer (nullable = true)\n |-- store_location: string (nullable = true)\n |-- forecasted_demand: integer (nullable = true)\n |-- promotion_applied: boolean (nullable = true)\n |-- promotion_type: string (nullable = true)\n |-- weather_conditions: string (nullable = true)\n |-- holiday_indicator: boolean (nullable = true)\n |-- weekday: string (nullable = true)\n |-- actual_demand: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Exibir o esquema do DataFrame filtrado\n",
    "df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4400b570-e4d5-4923-9beb-f86bf2bca929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exibição das Estatísticas Descritivas para as Colunas Numéricas\n",
    "\n",
    "Neste passo, exibimos as estatísticas descritivas das colunas numéricas do DataFrame filtrado. As estatísticas descritivas fornecem informações importantes sobre a distribuição e as características dos dados, como:\n",
    "\n",
    "- **count**: Número de valores não nulos.\n",
    "- **mean**: Média dos valores.\n",
    "- **stddev**: Desvio padrão, que indica a dispersão dos dados.\n",
    "- **min**: Valor mínimo.\n",
    "- **max**: Valor máximo.\n",
    "\n",
    "A função `describe()` é usada para calcular essas estatísticas, e o método `show()` é utilizado para exibir o resultado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b43823-6a2d-4498-a627-72f037ca0e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+--------------+-----------------+-------------------+------------------+---------+------------------+\n|summary|        product_id|     quantity_sold|   inventory_level|     reorder_point| reorder_quantity|         store_id|store_location|forecasted_demand|     promotion_type|weather_conditions|  weekday|     actual_demand|\n+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+--------------+-----------------+-------------------+------------------+---------+------------------+\n|  count|              5000|              5000|              5000|              5000|             5000|             5000|          5000|             5000|               5000|              5000|     5000|              5000|\n|   mean|          551.2334|            2.9828|          253.1218|            99.788|          200.517|           10.525|          null|          297.134|               null|              null|     null|          299.0884|\n| stddev|258.82660598716154|1.4194742482441463|142.88545557698686|29.132387392711905|58.25738146792301|5.786887990493933|          null|115.5688057988606|               null|              null|     null|121.68078002661545|\n|    min|               100|                 1|                 0|                50|              100|                1|   Chicago, IL|              100|               BOGO|            Cloudy|   Friday|                90|\n|    max|               999|                 5|               500|               150|              300|               20|  New York, NY|              500|Percentage Discount|             Sunny|Wednesday|               510|\n+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+--------------+-----------------+-------------------+------------------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Exibir estatísticas descritivas para as colunas numéricas\n",
    "df_filtered.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd572c16-b2ec-49ca-9234-229710ffad4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contagem de Valores Únicos por Coluna\n",
    "\n",
    "Neste passo, realizamos a contagem de valores únicos em uma coluna específica, neste caso, o número de registros por loja. A contagem de valores únicos é útil para entender a distribuição dos dados em uma determinada categoria, como a quantidade de registros por localização de loja.\n",
    "\n",
    "Usamos o método `groupBy()` para agrupar os dados pela coluna `store_location` e, em seguida, aplicamos a função `count()` para contar o número de registros em cada grupo. O método `show()` é utilizado para exibir os resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56d14f3-cce5-4242-ace7-b73743d11e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n| store_location|count|\n+---------------+-----+\n|Los Angeles, CA| 1038|\n|     Dallas, TX|  998|\n|    Chicago, IL| 1013|\n|   New York, NY|  987|\n|      Miami, FL|  964|\n+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Contar valores únicos em uma coluna, por exemplo, numero de registors por loja\n",
    "df_filtered.groupBy('store_location').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0703f962-7e7e-43d3-94bf-0de12967f3ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contagem de Valores Únicos na Coluna de Promoções Aplicadas\n",
    "\n",
    "Neste passo, contamos os valores únicos na coluna `promotion_applied`, que indica se uma promoção foi aplicada ou não. Essa análise é útil para entender quantas vezes as promoções foram aplicadas em relação ao total de registros.\n",
    "\n",
    "Utilizamos o método `groupBy()` para agrupar os dados pela coluna `promotion_applied` e, em seguida, aplicamos a função `count()` para contar o número de registros em cada grupo. O método `show()` é utilizado para exibir os resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8449d126-6e9b-4f72-8de2-596c3db97d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n|promotion_applied|count|\n+-----------------+-----+\n|             true| 2607|\n|            false| 2393|\n+-----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Contar valores únicos na coluna de promoções aplicadas\n",
    "df_filtered.groupBy('promotion_applied').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6c4fde-34ff-4780-baac-26a7487ecccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contagem de Valores Únicos na Coluna de Condição Climática\n",
    "\n",
    "Neste passo, contamos os valores únicos na coluna `weather_conditions`, que representa as diferentes condições climáticas presentes no conjunto de dados. Essa análise pode ser útil para verificar como diferentes condições climáticas afetam as vendas ou a demanda de produtos.\n",
    "\n",
    "Usamos o método `groupBy()` para agrupar os dados pela coluna `weather_conditions` e, em seguida, aplicamos a função `count()` para contar o número de registros em cada condição climática. O método `show()` é utilizado para exibir os resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1faf0d72-e4d7-4631-87fe-938d1bc49134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n|weather_conditions|count|\n+------------------+-----+\n|            Cloudy| 1281|\n|             Sunny| 1268|\n|             Rainy| 1218|\n|            Stormy| 1233|\n+------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Contar valores únicos na coluna de condição do clima\n",
    "df_filtered.groupBy('weather_conditions').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305bfa20-043d-49d9-b1b5-e38adba5a96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contagem de Valores Únicos na Coluna de Dias da Semana\n",
    "\n",
    "Neste passo, contamos os valores únicos na coluna `weekday`, que representa os dias da semana em que as transações ocorreram. Essa análise pode ser útil para identificar padrões de comportamento, como quais dias da semana têm mais vendas ou demanda.\n",
    "\n",
    "Usamos o método `groupBy()` para agrupar os dados pela coluna `weekday` e, em seguida, aplicamos a função `count()` para contar o número de registros em cada dia da semana. O método `show()` é utilizado para exibir os resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5722c1f6-0350-49de-b0ec-37c974345179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|  weekday|count|\n+---------+-----+\n|Wednesday|  691|\n|  Tuesday|  736|\n|   Friday|  668|\n| Thursday|  760|\n| Saturday|  710|\n|   Monday|  775|\n|   Sunday|  660|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Contar valores únicos na coluna de dias da semana\n",
    "df_filtered.groupBy('weekday').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28249159-ee93-4674-a382-c00df1280fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contagem de Valores Distintos na Coluna de Quantidade Vendida\n",
    "\n",
    "Neste passo, exibimos os valores distintos da coluna `quantity_sold`, que representa a quantidade de unidades vendidas. Verificar os valores distintos ajuda a entender a variação nas quantidades de produtos vendidos, identificando se há valores duplicados ou se existem quantidades únicas de vendas.\n",
    "\n",
    "Usamos o método `select()` para selecionar a coluna `quantity_sold`, seguido de `distinct()` para obter os valores únicos dessa coluna. O método `show()` é utilizado para exibir os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef723299-935e-4987-b124-fcbe9677d0e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|quantity_sold|\n+-------------+\n|            1|\n|            3|\n|            5|\n|            4|\n|            2|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Conta valores distintos na coluna de quantidade vendida\n",
    "df_filtered.select(\"quantity_sold\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b7212e-06ca-4b40-af0a-f42e9dcd311d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Identificação dos Produtos Mais Vendidos por Localização de Loja\n",
    "\n",
    "Neste passo, estamos agrupando os dados por `store_location` (localização da loja) e `product_id` (identificador do produto) para calcular o total de unidades vendidas de cada produto em cada loja. Esse processo permite identificar quais produtos são mais vendidos em cada localização, uma informação importante para a gestão de estoque e planejamento de demanda.\n",
    "\n",
    "Utilizamos a função `groupBy()` para agrupar os dados pelas colunas `store_location` e `product_id`. Em seguida, usamos `agg()` com a função `F.sum()` para somar a quantidade vendida de cada produto. O resultado é ordenado pela localização da loja e pela quantidade total vendida, de forma decrescente, usando o método `orderBy()`.\n",
    "\n",
    "O método `show(10)` exibe os 10 primeiros resultados, apresentando as lojas e os produtos mais vendidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cccee652-b178-4ced-8a5f-f9435ae34ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------------------+\n|store_location|product_id|total_quantity_sold|\n+--------------+----------+-------------------+\n|  New York, NY|       251|                 20|\n|  New York, NY|       154|                 17|\n|  New York, NY|       314|                 17|\n|  New York, NY|       233|                 16|\n|  New York, NY|       668|                 16|\n|  New York, NY|       167|                 16|\n|  New York, NY|       587|                 16|\n|  New York, NY|       760|                 15|\n|  New York, NY|       108|                 14|\n|  New York, NY|       279|                 14|\n+--------------+----------+-------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Identifica os produtos mais vendidos por localizaçao\n",
    "produtos_mais_vendidos = df_filtered.groupBy(\"store_location\", \"product_id\") \\\n",
    "    .agg(F.sum(\"quantity_sold\").alias(\"total_quantity_sold\")) \\\n",
    "    .orderBy(\"store_location\", \"total_quantity_sold\", ascending=False)\n",
    "\n",
    "produtos_mais_vendidos.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edfc4d6f-d6dd-4642-96f1-440d3892974d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Criação do DataFrame de Estoque e Demanda\n",
    "\n",
    "Neste passo, criamos uma nova coluna no DataFrame `df_filtered` para verificar se o estoque é suficiente para atender à demanda real de cada produto. A coluna `estoque_suficiente` será preenchida com `True` se o nível de estoque (`inventory_level`) for maior ou igual à demanda real (`actual_demand`), e `False` caso contrário. Essa verificação é fundamental para a gestão de estoque, permitindo identificar produtos com risco de falta de estoque.\n",
    "\n",
    "Usamos a função `withColumn()` para adicionar a nova coluna, aplicando a lógica condicional com a função `when()` para comparar o nível de estoque com a demanda real. A expressão `when()` cria uma coluna booleana, retornando `True` ou `False` com base na condição.\n",
    "\n",
    "Em seguida, usamos `select()` para exibir as colunas relevantes e confirmar o resultado da verificação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc32460-a4b5-47d0-90ee-a0395362a220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------------+-------------+------------------+\n| store_location|product_id|inventory_level|actual_demand|estoque_suficiente|\n+---------------+----------+---------------+-------------+------------------+\n|Los Angeles, CA|       405|             77|          299|             false|\n|Los Angeles, CA|       987|            447|          246|              true|\n|      Miami, FL|       165|            204|          315|             false|\n|      Miami, FL|       516|            126|          357|             false|\n|     Dallas, TX|       472|            160|          413|             false|\n|Los Angeles, CA|       787|            220|          282|             false|\n|     Dallas, TX|       773|             12|          178|             false|\n|    Chicago, IL|       192|             17|          483|             false|\n|      Miami, FL|       506|            196|          379|             false|\n|   New York, NY|       959|            331|          467|             false|\n+---------------+----------+---------------+-------------+------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Criar o DataFrame estoque_Demanda com a lógica de verificação de estoque suficiente\n",
    "estoque_Demanda = df_filtered.withColumn(\"estoque_suficiente\", when(col(\"inventory_level\") >= col(\"actual_demand\"), True).otherwise(False))\n",
    "\n",
    "# Exibir as colunas desejadas para verificação\n",
    "estoque_Demanda.select(\"store_location\", \"product_id\", \"inventory_level\", \"actual_demand\", \"estoque_suficiente\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba82197e-88d6-458e-9780-c70f6e75304c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Identificação de Estoques Insuficientes\n",
    "\n",
    "Nesta seção, analisamos os itens com estoque insuficiente em relação à demanda real, destacando a magnitude dessa insuficiência por meio de uma nova métrica chamada **defasagem**.\n",
    "\n",
    "### Passos:\n",
    "1. **Filtragem de Estoques Insuficientes**:\n",
    "   - Utilizamos o DataFrame `estoque_Demanda` para filtrar os itens em que o estoque disponível não é suficiente para atender à demanda real.\n",
    "   - A condição de insuficiência é definida pela coluna `estoque_suficiente` sendo igual a `False`.\n",
    "\n",
    "2. **Cálculo da Defasagem**:\n",
    "   - Adicionamos uma nova coluna chamada `defasagem`, que calcula a diferença entre a demanda real (`actual_demand`) e o nível de estoque disponível (`inventory_level`).\n",
    "   - A fórmula utilizada foi:  \n",
    "     **defasagem = actual_demand - inventory_level**\n",
    "\n",
    "3. **Exibição dos Dados**:\n",
    "   - Visualizamos as primeiras 10 linhas do DataFrame resultante, destacando as colunas mais relevantes:  \n",
    "     - `store_location`: Localização da loja.\n",
    "     - `product_id`: Identificador do produto.\n",
    "     - `inventory_level`: Estoque disponível.\n",
    "     - `actual_demand`: Demanda real.\n",
    "     - `defasagem`: Quantidade insuficiente de estoque.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1fd747f-8078-41fc-a6c6-c26c9028ba7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------------+-------------+---------+\n| store_location|product_id|inventory_level|actual_demand|defasagem|\n+---------------+----------+---------------+-------------+---------+\n|Los Angeles, CA|       405|             77|          299|      222|\n|      Miami, FL|       165|            204|          315|      111|\n|      Miami, FL|       516|            126|          357|      231|\n|     Dallas, TX|       472|            160|          413|      253|\n|Los Angeles, CA|       787|            220|          282|       62|\n|     Dallas, TX|       773|             12|          178|      166|\n|    Chicago, IL|       192|             17|          483|      466|\n|      Miami, FL|       506|            196|          379|      183|\n|   New York, NY|       959|            331|          467|      136|\n|Los Angeles, CA|       841|             95|          421|      326|\n+---------------+----------+---------------+-------------+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filtra o DataFrame para encontrar os itens com estoque insuficiente\n",
    "estoque_Insuficiente_df = estoque_Demanda.filter(estoque_Demanda.estoque_suficiente == False)\n",
    "\n",
    "# Adiciona uma nova coluna \"defasagem\" calculando a diferença entre a demanda e o estoque\n",
    "estoque_Insuficiente_df = estoque_Insuficiente_df.withColumn(\"defasagem\", F.col(\"actual_demand\") - F.col(\"inventory_level\"))\n",
    "\n",
    "# Mostra as primeiras 10 linhas do DataFrame com a coluna \"defasagem\"\n",
    "estoque_Insuficiente_df.select(\"store_location\", \"product_id\", \"inventory_level\", \"actual_demand\", \"defasagem\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e5ddad5-ebb4-40f7-bee9-e44ec9220c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ordenação dos Produtos com Maior Defasagem de Estoque\n",
    "\n",
    "Neste passo, estamos ordenando os produtos com maior defasagem de estoque. A defasagem é calculada com base na diferença entre o nível de estoque e a demanda real de cada produto. Produtos com maior defasagem indicam que o estoque não é suficiente para atender à demanda e podem precisar ser priorizados para reposição.\n",
    "\n",
    "Usamos o método `orderBy()` para ordenar os produtos pela coluna `defasagem` em ordem decrescente, ou seja, do produto com maior defasagem para o menor. A função `select()` é utilizada para exibir as colunas relevantes, como a localização da loja, o identificador do produto, o nível de estoque, a demanda real e a defasagem.\n",
    "\n",
    "O comando `show(10)` exibe os 10 produtos com maior defasagem, permitindo uma visualização rápida dos itens que mais necessitam de reposição.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2d4655-a514-4b19-ad84-82604d0b3eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------------+-------------+---------+\n| store_location|product_id|inventory_level|actual_demand|defasagem|\n+---------------+----------+---------------+-------------+---------+\n|   New York, NY|       342|              1|          504|      503|\n|Los Angeles, CA|       469|              3|          503|      500|\n|     Dallas, TX|       973|             16|          509|      493|\n|Los Angeles, CA|       943|             15|          507|      492|\n|    Chicago, IL|       607|             13|          504|      491|\n|     Dallas, TX|       979|             18|          506|      488|\n|     Dallas, TX|       743|             21|          508|      487|\n|   New York, NY|       955|              9|          495|      486|\n|     Dallas, TX|       801|              0|          483|      483|\n|    Chicago, IL|       713|             21|          503|      482|\n+---------------+----------+---------------+-------------+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Ordenar os produtos com maior defasagem\n",
    "defasagem_ordenada_df = estoque_Insuficiente_df.orderBy(\"defasagem\", ascending=False)\n",
    "\n",
    "# Mostrar os 10 produtos com maior defasagem\n",
    "defasagem_ordenada_df.select(\"store_location\", \"product_id\", \"inventory_level\", \"actual_demand\", \"defasagem\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547b852d-48eb-4eec-8357-352c618c81bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Filtragem de Dados de Promoções Aplicadas e Análise de Defasagem\n",
    "\n",
    "Neste passo, adicionamos uma nova coluna chamada `promotion_status` para indicar se uma promoção foi aplicada ou não aos produtos. O objetivo é analisar o impacto das promoções em produtos que estão com defasagem de estoque. Produtos com promoção aplicada e com estoque insuficiente podem exigir um planejamento especial para garantir que o estoque esteja adequado para atender à demanda gerada pela promoção.\n",
    "\n",
    "Usamos o método `withColumn()` para criar a coluna `promotion_status`, que recebe o valor \"Promoção Aplicada\" quando a coluna `promotion_applied` for `True` e \"Sem Promoção\" caso contrário. \n",
    "\n",
    "Em seguida, utilizamos o método `select()` para exibir as colunas relevantes: a localização da loja, o identificador do produto, o status da promoção, a demanda real e a defasagem de estoque. Os resultados são ordenados pela coluna `defasagem` de forma **decrescente** utilizando o método `orderBy()`, permitindo que os produtos com maior defasagem de estoque apareçam primeiro.\n",
    "\n",
    "O comando `show(10)` exibe os 10 primeiros resultados, facilitando a visualização dos produtos com promoções e suas respectivas defasagens de estoque.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2258ff-5fc6-4a4e-af67-6b9f7e9fef6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------------+-------------+---------+\n| store_location|product_id| promotion_status|actual_demand|defasagem|\n+---------------+----------+-----------------+-------------+---------+\n|   New York, NY|       342|Promoção Aplicada|          504|      503|\n|Los Angeles, CA|       469|     Sem Promoção|          503|      500|\n|     Dallas, TX|       973|Promoção Aplicada|          509|      493|\n|Los Angeles, CA|       943|Promoção Aplicada|          507|      492|\n|    Chicago, IL|       607|Promoção Aplicada|          504|      491|\n|     Dallas, TX|       979|     Sem Promoção|          506|      488|\n|     Dallas, TX|       743|     Sem Promoção|          508|      487|\n|   New York, NY|       955|Promoção Aplicada|          495|      486|\n|     Dallas, TX|       801|Promoção Aplicada|          483|      483|\n|    Chicago, IL|       713|     Sem Promoção|          503|      482|\n+---------------+----------+-----------------+-------------+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Adicionar uma nova coluna 'promotion_status' para indicar se a promoção foi aplicada ou não\n",
    "impacto_promocao_df = estoque_Insuficiente_df.withColumn(\"promotion_status\",when(estoque_Insuficiente_df.promotion_applied == True, \"Promoção Aplicada\").otherwise(\"Sem Promoção\")\n",
    ")\n",
    "\n",
    "# Exibir os dados, incluindo a nova coluna 'promotion_status'\n",
    "impacto_promocao_df.select(\"store_location\", \"product_id\", \"promotion_status\", \"actual_demand\", \"defasagem\").orderBy(\"defasagem\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3b6f41-e017-45a9-a284-ba9a7ed04744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análise da Média da Defasagem por Tipo de Promoção\n",
    "\n",
    "Neste passo, estamos analisando a média da defasagem de estoque para os diferentes tipos de promoção. A ideia é entender se há uma diferença significativa na defasagem entre os produtos que receberam diferentes tipos de promoção.\n",
    "\n",
    "Usamos o método `groupBy()` para agrupar os dados pela coluna `promotion_type`, que representa o tipo de promoção aplicada. Em seguida, utilizamos `agg()` com a função `F.avg()` para calcular a média da defasagem para cada tipo de promoção. O comando `show()` exibe os resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916c7de0-12ba-4a3b-8f1d-9485b4b94929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n|     promotion_type|   media_defasagem|\n+-------------------+------------------+\n|               None|170.61284046692606|\n|Percentage Discount|176.64380530973452|\n|               BOGO|  162.798755186722|\n+-------------------+------------------+\n\n+----------------------------+\n|media_defasagem_sem_promocao|\n+----------------------------+\n|          170.01760563380282|\n+----------------------------+\n\n+----------------------------+\n|media_defasagem_com_promocao|\n+----------------------------+\n|          170.48853503184714|\n+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Analisando a média da defasagem por tipo de promoção\n",
    "impacto_promocao_df.groupBy(\"promotion_type\").agg(F.avg(\"defasagem\").alias(\"media_defasagem\")).show()\n",
    "\n",
    "# Comparando produtos com e sem promoção\n",
    "estoque_sem_promocao_df = estoque_Insuficiente_df.filter(estoque_Insuficiente_df.promotion_applied == False)\n",
    "estoque_com_promocao_df = estoque_Insuficiente_df.filter(estoque_Insuficiente_df.promotion_applied == True)\n",
    "\n",
    "# Média da defasagem para produtos sem promoção\n",
    "estoque_sem_promocao_df.agg(F.avg(\"defasagem\").alias(\"media_defasagem_sem_promocao\")).show()\n",
    "\n",
    "# Média da defasagem para produtos com promoção\n",
    "estoque_com_promocao_df.agg(F.avg(\"defasagem\").alias(\"media_defasagem_com_promocao\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d366880-80b0-416d-8c52-2430c81ac980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Produtos com Maior Defasagem e Previsão de Demanda Mais Alta\n",
    "\n",
    "Neste passo, estamos analisando os produtos com maior defasagem de estoque, especificamente aqueles com uma defasagem superior a 100 unidades. A ideia é identificar produtos que não só estão com estoque insuficiente, mas também têm uma previsão de demanda (`forecasted_demand`) alta, o que pode indicar um risco ainda maior de falta de estoque no futuro.\n",
    "\n",
    "Utilizamos o método `filter()` para selecionar os produtos com uma defasagem superior a 100. Em seguida, aplicamos o `select()` para exibir as colunas relevantes, incluindo a localização da loja, o identificador do produto, o nível de estoque, a demanda real, a previsão de demanda e a defasagem. Para facilitar a análise, os resultados são ordenados pela previsão de demanda e pela defasagem, de forma **decrescente**.\n",
    "\n",
    "O comando `show(10)` exibe os 10 primeiros resultados, permitindo uma visão clara dos produtos com maior risco de falta de estoque devido à alta demanda prevista e à defasagem de estoque.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e1a456-ee77-47e5-b333-20f6909f179b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------------+-------------+-----------------+---------+\n| store_location|product_id|inventory_level|actual_demand|forecasted_demand|defasagem|\n+---------------+----------+---------------+-------------+-----------------+---------+\n|   New York, NY|       874|             37|          504|              500|      467|\n|   New York, NY|       645|             80|          457|              500|      377|\n|   New York, NY|       560|             94|          467|              500|      373|\n|    Chicago, IL|       200|            147|          422|              500|      275|\n|     Dallas, TX|       225|             22|          263|              500|      241|\n|Los Angeles, CA|       766|            176|          412|              500|      236|\n|   New York, NY|       645|            121|          327|              500|      206|\n|   New York, NY|       979|            291|          472|              500|      181|\n|    Chicago, IL|       248|            347|          500|              500|      153|\n|    Chicago, IL|       103|             41|          497|              499|      456|\n+---------------+----------+---------------+-------------+-----------------+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Produtos com maior defasagem e previsão de demanda mais alta\n",
    "estoque_Insuficiente_df.filter(estoque_Insuficiente_df.defasagem > 100).select(\"store_location\", \"product_id\", \"inventory_level\", \"actual_demand\", \"forecasted_demand\", \"defasagem\").orderBy([\"forecasted_demand\", \"defasagem\"], ascending=[False, False]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334d8f4c-87bb-4094-9a25-fe8bfd9fc3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análise de Erro na Previsão de Demanda\n",
    "\n",
    "Neste passo, estamos analisando a acurácia da previsão de demanda comparando a diferença entre a demanda real e a previsão de demanda. O objetivo é identificar os produtos cujas previsões estão dentro de uma margem de erro aceitável e os produtos com previsões significativamente imprecisas.\n",
    "\n",
    "### Definindo a Margem de Erro Aceitável\n",
    "Primeiro, definimos uma margem de erro de 20 unidades. Isso significa que qualquer diferença entre a demanda real e a previsão de demanda abaixo ou igual a 20 é considerada um \"acerto\", e qualquer diferença superior a 20 é considerada um \"erro\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8b0b2c-f981-4ecb-b99d-85c9a15cb315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produtos com previsão correta (dentro da margem de erro de 20): 461\nProdutos com previsão errada (fora da margem de erro de 20): 4539\n"
     ]
    }
   ],
   "source": [
    "# Definindo a margem de erro aceitável\n",
    "margem_erro = 20\n",
    "\n",
    "# Calcular a diferença entre demanda real e previsão de demanda\n",
    "estoque_Demanda = estoque_Demanda.withColumn(\"erro_previsao\", F.abs(col(\"actual_demand\") - col(\"forecasted_demand\")))\n",
    "\n",
    "# Separar os produtos em acertos e erros\n",
    "acertos_df = estoque_Demanda.filter(estoque_Demanda.erro_previsao <= margem_erro)\n",
    "erros_df = estoque_Demanda.filter(estoque_Demanda.erro_previsao > margem_erro)\n",
    "\n",
    "# Contar os acertos e erros\n",
    "acertos = acertos_df.count()\n",
    "erros = erros_df.count()\n",
    "\n",
    "# Exibir o resultado\n",
    "print(f\"Produtos com previsão correta (dentro da margem de erro de {margem_erro}): {acertos}\")\n",
    "print(f\"Produtos com previsão errada (fora da margem de erro de {margem_erro}): {erros}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b176563-1bfe-4bc2-bea1-f4e5308936a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análise da Defasagem por Dia da Semana\n",
    "\n",
    "Neste passo, estamos analisando a média da defasagem de estoque por dia da semana. A ideia é verificar se há variações na defasagem ao longo da semana, o que pode ajudar a entender melhor os padrões de demanda e estoque com base no dia.\n",
    "\n",
    "### Agrupamento por Dia da Semana\n",
    "Usamos o método `groupBy()` para agrupar os dados pela coluna `weekday`, que representa o dia da semana. Em seguida, aplicamos a função `agg()` com a função `F.avg()` para calcular a média da defasagem para cada dia da semana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18369402-2933-4e3f-92ab-52aaba52ab20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n|  weekday|   media_defasagem|\n+---------+------------------+\n|Wednesday|175.16497461928935|\n|  Tuesday| 169.7454954954955|\n|   Friday|173.67401960784315|\n| Thursday|168.56415929203538|\n| Saturday|166.25943396226415|\n|   Monday|172.46103896103895|\n|   Sunday|166.22906403940885|\n+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por dia da semana e verificar a média da defasagem\n",
    "estoque_Insuficiente_df.groupBy(\"weekday\").agg(F.avg(\"defasagem\").alias(\"media_defasagem\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00922ef-e0bc-4609-8ba3-976c37ba0b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análise do Impacto das Condições Meteorológicas e Feriados na Defasagem\n",
    "\n",
    "Neste passo, estamos analisando o impacto das condições meteorológicas e dos feriados na defasagem de estoque. A ideia é verificar se as condições externas, como o clima e a ocorrência de feriados, têm influência sobre a diferença entre a demanda real e o estoque disponível.\n",
    "\n",
    "### Agrupamento por Condições Meteorológicas e Feriados\n",
    "Usamos o método `groupBy()` para agrupar os dados pelas colunas `weather_conditions` (condições meteorológicas) e `holiday_indicator` (indicador de feriado). Em seguida, aplicamos a função `agg()` com a função `F.avg()` para calcular a média da defasagem para cada combinação de condições meteorológicas e feriados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ec358a-3830-4a14-b7c6-573c94f57b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+\n|weather_conditions|holiday_indicator|   media_defasagem|\n+------------------+-----------------+------------------+\n|             Rainy|             true| 165.9947780678851|\n|            Cloudy|             true|178.13774104683196|\n|            Cloudy|            false|176.97948717948717|\n|            Stormy|             true| 171.3844155844156|\n|            Stormy|            false| 169.9611111111111|\n|             Sunny|            false|160.55555555555554|\n|             Rainy|            false|173.49142857142857|\n|             Sunny|             true|166.42372881355934|\n+------------------+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar o impacto das condições meteorológicas e feriados na defasagem\n",
    "estoque_Insuficiente_df.groupBy(\"weather_conditions\", \"holiday_indicator\").agg(F.avg(\"defasagem\").alias(\"media_defasagem\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d89bee-b45e-4cc7-ba55-1f4148a60606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusão\n",
    "\n",
    "Este projeto apresentou uma análise detalhada sobre a gestão de estoque e o impacto das promoções nas vendas de produtos, com foco em identificar as oportunidades de melhoria na previsão de demanda e na adequação do estoque. A seguir, estão os principais insights obtidos a partir dos dados analisados:\n",
    "\n",
    "1. **Produtos Mais Vendidos**: Identificamos os produtos que tiveram as maiores quantidades vendidas nas lojas, o que ajuda a priorizar o abastecimento e garantir que itens populares estejam sempre disponíveis para os clientes.\n",
    "\n",
    "2. **Produtos com Maior Defasagem de Estoque**: A análise de defasagem revelou os produtos com a maior diferença entre a quantidade de estoque disponível e a demanda real. Estes itens exigem atenção imediata para evitar rupturas de estoque que possam impactar negativamente as vendas e a satisfação do cliente.\n",
    "\n",
    "3. **Impacto das Promoções nas Defasagens de Estoque**: A filtragem de dados revelou a relação entre as promoções e a defasagem de estoque. Produtos com promoções aplicadas tendem a apresentar uma maior defasagem, o que sugere que as promoções podem estar gerando uma demanda não totalmente atendida. Isso destaca a importância de ajustar o estoque de forma mais eficaz antes de aplicar promoções, para que as lojas possam atender à demanda extra sem problemas.\n",
    "\n",
    "4. **Análise da Média de Defasagem por Tipo de Promoção**: A comparação entre os diferentes tipos de promoção mostrou que o \"Percentage Discount\" e \"BOGO\" (Buy One Get One) estão associados a médias de defasagem mais altas. Isso indica que as promoções desse tipo podem estar exacerbando a escassez de produtos e exigem maior controle no planejamento de estoques.\n",
    "\n",
    "5. **Produtos com Maior Defasagem e Previsão de Demanda Alta**: Identificamos produtos com previsão de alta demanda futura e alta defasagem de estoque. Esses produtos representam um risco significativo de rupturas no futuro, o que reforça a necessidade de estratégias de reposição de estoque mais precisas para atender à demanda futura e evitar vendas perdidas.\n",
    "\n",
    "6. **Análise da Previsão de Demanda**: A análise das previsões de demanda revelou que, embora muitas previsões estejam dentro da margem de erro aceitável (20 unidades), um grande número de produtos apresenta previsões incorretas, o que pode resultar em falhas no planejamento de estoque. Isso destaca a importância de melhorar a precisão da previsão para garantir um alinhamento mais eficiente entre a demanda e o estoque.\n",
    "\n",
    "### Recomendações:\n",
    "\n",
    "- **Ajuste de Estoque para Promoções**: É fundamental garantir que o estoque seja ajustado adequadamente antes da aplicação de promoções, considerando o aumento esperado na demanda.\n",
    "  \n",
    "- **Melhoria nas Previsões de Demanda**: O modelo de previsão de demanda precisa ser ajustado para melhorar sua precisão e reduzir a margem de erro, o que ajudará na melhor adequação dos estoques.\n",
    "  \n",
    "- **Foco em Produtos Críticos**: A priorização dos produtos com maior defasagem e previsão de alta demanda pode ajudar a evitar rupturas e maximizar as vendas.\n",
    "  \n",
    "- **Análise Contínua**: Uma análise contínua dos dados de vendas e estoque, juntamente com a revisão das previsões de demanda e impacto das promoções, ajudará a manter a eficiência operacional e a satisfação do cliente.\n",
    "\n",
    "Essa análise oferece uma visão estratégica valiosa para a otimização do gerenciamento de estoque e a melhoria contínua das operações de vendas nas lojas.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Analise_estoque_previsao_demanda",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
